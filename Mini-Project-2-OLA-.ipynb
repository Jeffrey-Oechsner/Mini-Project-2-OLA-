{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494d6043",
   "metadata": {},
   "source": [
    "Step 1: Obtaining and Preprocessing the Data\n",
    "\n",
    "These are the common libraries used in machine learning and data processing.\n",
    "\n",
    "NumPy is used for numerical operations on arrays.\n",
    "\n",
    "Pandas provides data structures and data analysis tools.\n",
    "\n",
    "Matplotlib is for plotting graphs and visualizing data.\n",
    "\n",
    "TensorFlow is an end-to-end open-source platform for machine learning.\n",
    "\n",
    "Keras is part of TensorFlow and provides tools for working with deep learning models.\n",
    "\n",
    "CIFAR-10 dataset is a classic dataset for image classification, which has 60,000 32x32 color images in 10 classes.\n",
    "\n",
    "The data is split into 50,000 training images and 10,000 test images.\n",
    "\n",
    "Normalizing the pixel values between 0 and 1 helps the CNN to train faster and reduce the chance of getting stuck in local optima.\n",
    "\n",
    "Encoding labels converts the categorical variable into a format that can be provided to ML algorithms to do a better job in prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6f039b-109b-44c0-a530-dfcfe0798e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 153s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset - here's an example with the CIFAR-10 dataset\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Encode labels if they're categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train.flatten())\n",
    "y_test = label_encoder.transform(y_test.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d921",
   "metadata": {},
   "source": [
    "Step 2: Building the CNN Model\n",
    "\n",
    "\n",
    "Sequential model is a linear stack of layers.\n",
    "\n",
    "Conv2D layers are convolutional layers that will extract features from the image.\n",
    "\n",
    "MaxPooling2D layers are used to reduce the spatial dimensions of the output volume.\n",
    "\n",
    "After convolutional layers, we flatten the output to feed it into dense layers for classification.\n",
    "\n",
    "Dense layers are the fully connected layers.\n",
    "\n",
    "The last layer has as many neurons as classes; it outputs the logits for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febd6453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "])\n",
    "\n",
    "# Add dense layers on top\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10)) # Assuming 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6efbf5",
   "metadata": {},
   "source": [
    "Step 3: Training the Model\n",
    "\n",
    "I train (fit) the model using the training data.\n",
    "\n",
    "Epochs refer to the number of times the model sees the entire dataset. Here, we use 10 for demonstration.\n",
    "\n",
    "Validation data is used to evaluate the model after each epoch of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44210c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5146 - accuracy: 0.4509 - val_loss: 1.2925 - val_accuracy: 0.5387\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1442 - accuracy: 0.5928 - val_loss: 1.0539 - val_accuracy: 0.6333\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0039 - accuracy: 0.6468 - val_loss: 1.0066 - val_accuracy: 0.6466\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.9049 - accuracy: 0.6815 - val_loss: 0.9563 - val_accuracy: 0.6701\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 0.8353 - accuracy: 0.7071 - val_loss: 0.9471 - val_accuracy: 0.6738\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7734 - accuracy: 0.7277 - val_loss: 0.8800 - val_accuracy: 0.6974\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7245 - accuracy: 0.7452 - val_loss: 0.8924 - val_accuracy: 0.6974\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 0.6757 - accuracy: 0.7626 - val_loss: 0.8832 - val_accuracy: 0.7060\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.6373 - accuracy: 0.7747 - val_loss: 0.8904 - val_accuracy: 0.7000\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 0.5962 - accuracy: 0.7914 - val_loss: 0.8940 - val_accuracy: 0.7105\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616193b",
   "metadata": {},
   "source": [
    "Step 4: Validation and Testing\n",
    "\n",
    "\n",
    "After training, we evaluate the model's performance on the test dataset.\n",
    "\n",
    "I plot the training and validation accuracy to monitor overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fe9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3cff7",
   "metadata": {},
   "source": [
    "Step 5: Quality Assessment and Discussion\n",
    "\n",
    "confusion_matrix computes the confusion matrix to evaluate the accuracy of a classification.\n",
    "\n",
    "sns.heatmap creates a heatmap for the confusion matrix, making it visually interpretable.\n",
    "\n",
    "classification_report provides a report on precision, recall, and F1 scores for each class.\n",
    "\n",
    "Remember to replace y_test and y_pred_classes with your actual test labels and predicted labels variables.\n",
    "\n",
    "For the discussion part:\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It shows how precise your model is out of those predicted positive, how many of them are actual positive.\n",
    "\n",
    "Recall (Sensitivity) is the ratio of correctly predicted positive observations to the all observations in actual class. It shows how many of the actual positives our model capture through labeling it as positive.\n",
    "\n",
    "F1 Score is the weighted average of Precision and Recall. This score takes both false positives and false negatives into account. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "And finally, when you compare your CNN model's performance to other classification algorithms, you can discuss aspects like:\n",
    "\n",
    "How does the CNN model's accuracy compare to more traditional algorithms on the same dataset?\n",
    "\n",
    "Do the precision, recall, and F1 scores indicate that the CNN is better at certain types of errors (false positives vs false negatives)?\n",
    "\n",
    "Consider the complexity of the CNN model in terms of training time and parameter tuning compared to other models. Is the increase in accuracy (if any) worth the additional complexity?\n",
    "\n",
    "These insights will not only help you evaluate your model but also help in understanding under which conditions CNNs might be preferred over other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a890296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the classes\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78ed24",
   "metadata": {},
   "source": [
    "let's calculate the confusion matrix and extract the precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f82993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (2.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeffo\\miniconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 112.6/294.9 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.9/294.9 kB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Install seaborn using pip\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report includes precision, recall, and F1 score\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76a43f",
   "metadata": {},
   "source": [
    "Discussion Questions\n",
    "\n",
    "Definitions of ANN and CNN based on your experience.\n",
    "    \n",
    "Hyperparameters you fine-tuned and their impact.\n",
    "    \n",
    "Role of activation functions and the ones you chose.\n",
    "    \n",
    "Advantages and disadvantages of using ANN and CNN for classification tasks.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075638a7",
   "metadata": {},
   "source": [
    "Answers to Discussion Questions\n",
    "\n",
    "a. Definition of ANN and CNN based on experience:\n",
    "ANN, or Artificial Neural Network, is a computing system inspired by the biological neural networks. It consists of interconnected groups of nodes, akin to the vast network of neurons in a brain. CNN, or Convolutional Neural Network, is a class of ANN that is specifically designed to recognize visual patterns directly from pixel images with minimal preprocessing. They are powerful for image classification because they can automatically and adaptively learn spatial hierarchies of features from input images.\n",
    "\n",
    "b. Hyperparameters tuned and their impact:\n",
    "In this model, we might adjust the number of convolutional layers, filter sizes, the number of filters, and the number of neurons in the dense layers. Each hyperparameter tuning can affect the model's ability to generalize. For instance, increasing the number of filters might help the model in learning more complex patterns, but it can also lead to overfitting if not managed properly with regularization techniques.\n",
    "\n",
    "c. Role of activation functions and choices made:\n",
    "Activation functions determine the output of a neural network node given an input or set of inputs. They introduce non-linear properties to the network, allowing it to learn more complex functions. For instance, the 'relu' activation function is used to add non-linearity after convolutional layers, and the softmax function is typically used in the output layer to provide probabilities of the input being in a particular class.\n",
    "\n",
    "d. Advantages and disadvantages of ANN and CNN for classification:\n",
    "ANNs and CNNs are powerful for classification tasks as they can learn patterns in the data directly from the inputs. However, they require large amounts of data to train and are often referred to as \"black boxes\" due to their lack of interpretability. They can also be computationally intensive compared to simpler models. Yet, their ability to learn hierarchical feature representations automatically is a significant advantage over traditional algorithms that require manual feature extraction.\n",
    "\n",
    "In comparison to other algorithms like decision trees or SVMs, ANNs, especially CNNs, tend to perform better on image data. However, they may not be the best choice for small or structured datasets where simpler models could suffice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
